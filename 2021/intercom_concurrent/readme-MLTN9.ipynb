{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "d568976d192056dbc40b4f20450c40693209c689431c06ee8cdbba3e96c4577d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Hito 9\n",
    "## Comprimiendo con `zlib`\n",
    "\n",
    "Para entender mejor cómo es un `chunk`, hemos hecho una ilustración simple:\n",
    "\n",
    "![estructura-de-un-chunk](https://github.com/RaquelGG/TM/blob/master/otros/transponer1.png?raw=true)\n",
    "### Método `pack()`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack(self, seq, chunk):"
   ]
  },
  {
   "source": [
    "Hemos decidido comprobar por nuestra cuenta la diferencia de eficiencia entre separar los canales de cada frame en python, de forma normal o usando `numpy`, para ello usamos `timeit`, que cronometra únicamente la parte `stmt` y en setup creamos un array aleatorio, todos del mismo tamaño."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "con python: 8.919459699995059 s\ncon numpy slice : 0.1722801000069012 s\ncon numpy transpose : 0.14907310000126017 s\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "py = timeit.timeit(stmt='test[::2]', setup='import os;test=os.urandom(2**15)')\n",
    "np_sliced = timeit.timeit(stmt='test[::2]', setup='import os;import numpy;test=numpy.frombuffer(os.urandom(2**15),dtype=\"int16\")')\n",
    "np_trasposed = timeit.timeit(stmt='test.transpose()', setup='import os;import numpy;test=numpy.frombuffer(os.urandom(2**15),dtype=\"int16\").reshape(-1, 2)')\n",
    "print(\"con python:\", py, \"s\")\n",
    "print(\"con numpy slice :\", np_sliced, \"s\")\n",
    "print(\"con numpy transpose :\", np_trasposed, \"s\")"
   ]
  },
  {
   "source": [
    "Para mejorar la comprensión del código, trasponemos la matriz `chunk`, pasará a ser:\n",
    "\n",
    "![estructura-de-un-chunk](https://github.com/RaquelGG/TM/blob/master/otros/transponer2.png?raw=true)\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[[1],\n",
       "        [1]],\n",
       "\n",
       "       [[2],\n",
       "        [2]],\n",
       "\n",
       "       [[3],\n",
       "        [3]]])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "import numpy as np\n",
    "chunk = np.array([[[1], [1]],\n",
    "                  [[2], [2]],\n",
    "                  [[3], [3]]])\n",
    "chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[[1, 2, 3],\n",
       "        [1, 2, 3]]])"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "chunk.transpose()"
   ]
  },
  {
   "source": [
    "De esta manera solo recorreríamos los canales y queda más legible a la hora de comprimir con `zlib.compress()`:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zlib\n",
    "\n",
    "compressed_channels = [zlib.compress(np.ascontiguousarray(channel), level=zlib.Z_BEST_COMPRESSION) for channel in chunk.transpose()]"
   ]
  },
  {
   "source": [
    "Codificamos los datos en una secuencia de bytes que pueden ser enviadas por UDP, añadiendo, adicionalmente, el tamaño del primer canal comprimido (para luego poder diferenciar los canales)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "pack_format = f\"HH{2*len(compressed_channels[0])}s{2*len(compressed_channels[1])}s\""
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "Y por último, empaquetamos los datos:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return struct.pack(\n",
    "    pack_format, \n",
    "    seq, \n",
    "    2*len(compressed_channels[0]), # tamaño del primer canal comprimido\n",
    "    *compressed_channels, # * es para compressed_channel[0], [1], ... (expande el array)\n",
    ")"
   ]
  },
  {
   "source": [
    "### Método `unpack()`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack(self, packed_chunk):"
   ]
  },
  {
   "source": [
    "Obtenemos el tamaño del primer canal y del segundo, que se calcula con la diferencia de `packed_chunk` y el tamaño del primer canal:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_channel_size, = struct.unpack(\"H\", packed_chunk[SEQ_NO_SIZE:2*SEQ_NO_SIZE])\n",
    "second_channel_size = len(packed_chunk) - first_channel_size - 2*SEQ_NO_SIZE"
   ]
  },
  {
   "source": [
    "Una vez sabemos el tamaño de ambos canales, podemos desempaquetarlo por completo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq, _, first_channel_bytes, second_channel_bytes = struct.unpack(\n",
    "    f\"HH{first_channel_size}s{second_channel_size}s\",\n",
    "    packed_chunk,\n",
    ")"
   ]
  },
  {
   "source": [
    "Descomprimimos los canales"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_channel = np.frombuffer(\n",
    "    zlib.decompress(first_channel_bytes), \n",
    "    dtype='int16',\n",
    ")   \n",
    "second_channel = np.frombuffer(\n",
    "    zlib.decompress(second_channel_bytes),\n",
    "    dtype='int16'\n",
    ")"
   ]
  },
  {
   "source": [
    "Y por último, volvemos a dejar el chunk con su forma original:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ascontiguousarray(np.concatenate((first_channel, second_channel)).reshape(2,-1).transpose())"
   ]
  },
  {
   "source": [
    "![estructura-de-un-chunk](https://github.com/RaquelGG/TM/blob/master/otros/transponer1.png?raw=true)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "\n",
    "## Pruebas\n",
    "En el método `pack()` hemos calculado la tasa de compresión:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"size\", size, \"bytes, compression rate\", \"{:.2f}%\".format(100*(1-size/4096)))\n",
    "        pack_format = f\"HH{2*len(compressed_channels[0])}s{2*len(compressed_channels[1])}s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Como resultado:\n",
    "size 3838 bytes, compression rate 6.30%\n",
    "size 3926 bytes, compression rate 4.15%\n",
    "size 3682 bytes, compression rate 10.11%\n",
    "size 3748 bytes, compression rate 8.50%\n",
    "size 3818 bytes, compression rate 6.79%\n",
    "size 3854 bytes, compression rate 5.91%\n",
    "size 3880 bytes, compression rate 5.27%\n",
    "size 3852 bytes, compression rate 5.96%\n",
    "size 3926 bytes, compression rate 4.15%\n",
    "size 3874 bytes, compression rate 5.42%\n",
    "size 3706 bytes, compression rate 9.52%"
   ]
  },
  {
   "source": [
    "En este caso, la compresión ha mejorado el tamaño de los paquetes, pero hay veces que tiene un resultado contrario al que queremos:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "size 4216 bytes, compression rate -2.93%\n",
    "size 4170 bytes, compression rate -1.81%\n",
    "size 4106 bytes, compression rate -0.24%"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "El tamaño del paquete aumenta."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}